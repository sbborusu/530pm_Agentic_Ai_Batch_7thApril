{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6s3N8-hGLDqz"
      ],
      "authorship_tag": "ABX9TyOnzc+hWspBxxZg8L2JuPhJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbborusu/530pm_Agentic_Ai_Batch_7thApril/blob/main/scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scikit Introduction\n",
        "\n",
        "1. Primarily designed for machine learning rather than pure statistical analysis.\n",
        "2. scikit-learn and sklearn refer to the same library.\n",
        "3. scikit-learn is the official name of the popular Python machine learning library.\n",
        "4. sklearn is simply the alias used when importing the library in Python (import sklearn)\n",
        "5. So, while you install it using pip install scikit-learn, you use import sklearn in your code.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6s3N8-hGLDqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install scikit-learn"
      ],
      "metadata": {
        "id": "j9qOeL5xLcgX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impute using SimpleImputer\n",
        "\n",
        "1. SimpleImputer module in scikit-learn allows you to fill missing values using statistical strategies Mean, Median, Mode, constant.\n",
        "\n"
      ],
      "metadata": {
        "id": "w-xbOOWY1-y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Sample dataset with missing values (NaN)\n",
        "data = np.array([[7, 2, np.nan],\n",
        "                 [4, np.nan, 6],\n",
        "                 [np.nan, 3, 8],\n",
        "                 [5, 6, 9]])\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "df = pd.DataFrame(data, columns=['Feature1', 'Feature2', 'Feature3'])\n",
        "# print(\"Original Data:\\n\", df)\n",
        "\n",
        "# Creates an imputer to fill missing values with the mean. Only Intializes the object 'Simple Imputer' ie., sets up the imputer with the specified strategyâ€”it does not perform the imputation immediately.\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Instead of calling fit() and then transform() separately, fit_transform() does both in a single step.\n",
        "# 1. Calculates the mean (or other chosen strategy) for each column with missing values.\n",
        "# 2. Replaces missing values with the computed mean.\n",
        "\n",
        "data_imputed = imputer.fit_transform(df) # imputes the entire dataframe and copies to a different dataframe. returns an nd-array\n",
        "\n",
        "# impute specific column in a dataframe and put it back in the same dataframe\n",
        "#df.iloc[:, 0:1] = imputer.fit_transform(df.iloc[:,0:1])\n",
        "# (or)\n",
        "#df['Feature2'] = imputer.fit_transform(df[['Feature2']])\n",
        "#print(df)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "imputed_df = pd.DataFrame(data_imputed, columns=df.columns)\n",
        "print(\"\\nData After Imputation:\\n\", imputed_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfArh_n4Lgo1",
        "outputId": "0fa19980-2175-48cb-d7a9-71f68870a67b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data After Imputation:\n",
            "    Feature1  Feature2  Feature3\n",
            "0  7.000000  2.000000  7.666667\n",
            "1  4.000000  3.666667  6.000000\n",
            "2  5.333333  3.000000  8.000000\n",
            "3  5.000000  6.000000  9.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical to Numerical data conversion"
      ],
      "metadata": {
        "id": "zl1kWJHm2NEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding Technique\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample dataset\n",
        "data = pd.DataFrame({'State': ['California', 'Texas', 'Georgia', 'Texas', 'California']})\n",
        "\n",
        "label = LabelEncoder()\n",
        "\n",
        "#In the case of LabelEncoder, fit_transform() performs two steps in one:\n",
        "#- fit(): It learns the unique categories in the column and assigns each a numerical label.\n",
        "#- transform(): It converts the categorical values into their corresponding numeric labels.\n",
        "\n",
        "data['State'] = label.fit_transform(data['State'])"
      ],
      "metadata": {
        "id": "JFGVF8n_N6Js"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the variables for training and testing."
      ],
      "metadata": {
        "id": "KVdXp0XVyUem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data\n",
        "X = [[25, 50000], [30, 60000], [35, 70000], [40, 80000], [45, 90000]]\n",
        "y = [0, 0, 0, 1, 1]  # 0 = not purchased, 1 = purchased\n",
        "\n",
        "#perform feature scaling to make the magnitude of all features similar\n",
        "scalar = StandardScaler()\n",
        "X = scalar.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #random_state ensures same data is used as split each time we run this code. 42 or any other\n",
        "# integer between 0 and 99 can be used. No special meaning to the integer value being used, but its a common and peculiar habbit in ML for programmers to use 42.\n",
        "\n",
        "print(\"X_train:\", X_train)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"X_test:\", X_test)\n",
        "print(\"y_test:\", y_test)"
      ],
      "metadata": {
        "id": "aYRlO6uS3yGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70466cec-77ce-4f9e-c005-45a976afd8e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: [[ 1.41421356  1.41421356]\n",
            " [ 0.          0.        ]\n",
            " [-1.41421356 -1.41421356]\n",
            " [ 0.70710678  0.70710678]]\n",
            "y_train: [1, 0, 0, 1]\n",
            "X_test: [[-0.70710678 -0.70710678]]\n",
            "y_test: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a modal using Logistic Regression algorithm"
      ],
      "metadata": {
        "id": "rTi-8MpfdSx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# Creating and training the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "ZCr7Kbg026BZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6201033a-dfe7-4e24-daeb-5964e482cf72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metrics (compare the results y-test (actual) and y-predict(predicted))"
      ],
      "metadata": {
        "id": "Qj15wcHK1VtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred)) # This tells you how many predictions were correct when compared with y_test. 1.0 = 100%\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred, labels=[1,0]))\n"
      ],
      "metadata": {
        "id": "dHUL1p8reT2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c190f87-399a-4efa-c98b-86ab97604777"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            " [[0 0]\n",
            " [0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pickle the model as an object into a file for reusability"
      ],
      "metadata": {
        "id": "anXXYTxatpwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# save the model as .pkl file\n",
        "with open('purchse_classifier.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# load the model when needed\n",
        "#with open('purchse_classifier.pkl', 'rb') as file:\n",
        "#    loaded_model = pickle.load(file)"
      ],
      "metadata": {
        "id": "WHH9Z5ld5rl3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5lNkXJ2bufU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}